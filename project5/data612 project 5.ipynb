{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 612 Project 5\n",
    "#### Group member : Wei Zhou / Mia Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouw/anaconda2/lib/python2.7/site-packages/pyspark/context.py:220: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ratings CSV\n",
    "ratings_df = spark.read.csv('../project4/ml-100k/u.data',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\"\\t\",           # char for separation\n",
    "                         inferSchema=True\n",
    "                           )  # do we infer schema or not ?\n",
    "ratings_df.withColumnRenamed(\"196\",\"user_id\")\\\n",
    "                      .withColumnRenamed(\"242\",\"movie_id\")\\\n",
    "                      .withColumnRenamed(\"3\",\"rating\")\\\n",
    "                      .withColumnRenamed(\"881250949\",\"timestamp\")\\\n",
    "                      .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[297] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "Got 99999 ratings from 943 users on 1682 movies.\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings_df.rdd\n",
    "\n",
    "print(ratings)\n",
    "\n",
    "numRatings = ratings.count()\n",
    "numUsers = ratings.map(lambda r: r[0]).distinct().count()\n",
    "numMovies = ratings.map(lambda r: r[1]).distinct().count()\n",
    "\n",
    "print(\"Got %d ratings from %d users on %d movies.\" % (numRatings, numUsers, numMovies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- Toy Story (1995): string (nullable = true)\n",
      " |-- 01-Jan-1995: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- http://us.imdb.com/M/title-exact?Toy%20Story%20(1995): string (nullable = true)\n",
      " |-- 05: integer (nullable = true)\n",
      " |-- 06: integer (nullable = true)\n",
      " |-- 07: integer (nullable = true)\n",
      " |-- 18: integer (nullable = true)\n",
      " |-- 19: integer (nullable = true)\n",
      " |-- 110: integer (nullable = true)\n",
      " |-- 011: integer (nullable = true)\n",
      " |-- 012: integer (nullable = true)\n",
      " |-- 013: integer (nullable = true)\n",
      " |-- 014: integer (nullable = true)\n",
      " |-- 015: integer (nullable = true)\n",
      " |-- 016: integer (nullable = true)\n",
      " |-- 017: integer (nullable = true)\n",
      " |-- 018: integer (nullable = true)\n",
      " |-- 019: integer (nullable = true)\n",
      " |-- 020: integer (nullable = true)\n",
      " |-- 021: integer (nullable = true)\n",
      " |-- 022: integer (nullable = true)\n",
      " |-- 023: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read movies CSV\n",
    "movies_df = spark.read.csv('../project4/ml-100k/u.item',\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\"|\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "movies_df=movies_df.withColumnRenamed(\"10\",\"movie_id\")\n",
    "movies_df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| 242|count|\n",
      "+----+-----+\n",
      "| 496|  231|\n",
      "| 471|  221|\n",
      "| 463|   71|\n",
      "| 148|  128|\n",
      "|1342|    2|\n",
      "| 833|   49|\n",
      "|1088|   13|\n",
      "|1591|    6|\n",
      "|1238|    8|\n",
      "|1580|    1|\n",
      "|1645|    1|\n",
      "| 392|   68|\n",
      "| 623|   39|\n",
      "| 540|   43|\n",
      "| 858|    3|\n",
      "| 737|   59|\n",
      "| 243|  132|\n",
      "|1025|   44|\n",
      "|1084|   21|\n",
      "|1127|   11|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_counts = ratings_df.groupBy(\"242\").count()\n",
    "movies_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[196: int, 242: int, 3: int, 881250949: int]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df, validation_df, test_df = ratings_df.randomSplit([.6, .2, .2])\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the diffrenet parameters which we can tune for the ALS model.\n",
    "seed = 5\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = range(4, 12)\n",
    "errors = []\n",
    "err = 0\n",
    "tolerance = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.5392303466796875), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.1617956161499023), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.130164384841919), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=4.114264011383057), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.5527002811431885)]\n",
      "For rank 4 the RMSE is 0.942490275965\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.5870258808135986), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.195436954498291), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.135669231414795), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=3.890799045562744), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.3917620182037354)]\n",
      "For rank 5 the RMSE is 0.941244095705\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.599729061126709), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.1440048217773438), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.025484800338745), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=4.026905536651611), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.566241979598999)]\n",
      "For rank 6 the RMSE is 0.944946325314\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.619906425476074), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.008010149002075), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.328622341156006), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=3.858994722366333), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.2605502605438232)]\n",
      "For rank 7 the RMSE is 0.94690855877\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.563685894012451), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.162628173828125), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.413465976715088), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=3.981210947036743), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.4696943759918213)]\n",
      "For rank 8 the RMSE is 0.946156225223\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.618748903274536), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.071305751800537), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.323016881942749), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=3.9671897888183594), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.4742562770843506)]\n",
      "For rank 9 the RMSE is 0.950086641817\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.4333887100219727), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.3234338760375977), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.376124382019043), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=3.9571478366851807), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.3146064281463623)]\n",
      "For rank 10 the RMSE is 0.951931983659\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(196=606, 242=148, 3=3, 881250949=878150506, prediction=3.7098944187164307), Row(196=663, 242=148, 3=4, 881250949=889492989, prediction=3.1198246479034424), Row(196=222, 242=148, 3=2, 881250949=881061164, prediction=3.4351613521575928), Row(196=330, 242=148, 3=4, 881250949=876544781, prediction=4.040489673614502), Row(196=224, 242=148, 3=3, 881250949=888104154, prediction=3.2846148014068604)]\n",
      "For rank 11 the RMSE is 0.953763934152\n",
      "The best model was trained with rank 5\n"
     ]
    }
   ],
   "source": [
    "#####Let's see which rank value will be the most optimized one .\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "\n",
    "for rank in ranks:\n",
    "    als = ALS(maxIter=iterations, regParam=regularization_parameter, rank=rank, userCol=\"196\", itemCol=\"242\", ratingCol=\"3\")\n",
    "    model = als.fit(training_df)\n",
    "    predictions = model.transform(validation_df)\n",
    "    print(type(predictions))\n",
    "    predictions.describe()\n",
    "    print(predictions.head(5))\n",
    "    new_predictions = predictions[predictions['prediction'] != np.NaN]\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"3\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(new_predictions)\n",
    "    errors.append(rmse)\n",
    "\n",
    "    print('For rank %s the RMSE is %s' % (rank, rmse))\n",
    "    if rmse < min_error:\n",
    "        min_error = rmse\n",
    "        best_rank = rank\n",
    "print('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank = 4 has the rmse = 0.942490275965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
