{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scale, you must understand the recommender system at multiple levels, from the algorithm complexity, the technique used and how to fight with things like network latency or I/O waiting times. The video compares about about different approaches to algorithms in Spark that can scale better at the expense of worker infrastucture and more maintenance around resolvers for different data types coming from python. \n",
    "\n",
    "Spark served well in this case replacing Hadoop because the algorithm processing time went down 70%, and combined with different implementations of the algorithm (graph lab, mllab, etc) could generate different results. I believe what I can take from this conversation is even different versions of a library in the same language can vary, so an understanding of the algorithm and how to play with variations will be paramount to enhance the productivity of the whole recommender system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
